---
layout: post
title: "Family Memories: Searchable Interviews with RAG"
subtitle: "A bilingual pipeline for recalling stories, building questions, and exploring shared history"
categories: nlp llm
permalink: /projects/family-memories-rag.html
---

# ü´∂ Family Memories: A Bilingual AI System to Explore and Revisit Family Interviews  
[View on Kaggle ‚Üí](https://www.kaggle.com/code/arturomp/family-memories)

(üá≤üáΩ en espa√±ol m√°s abajo üëá)

I have several recorded family interviews: conversations where I ask older relatives about their lives. They are incredibly valuable to me, to them and to our entire family, and I plan to do many more for as long as I can. But since this is a years-long effort, it's hard to remember what things were said and by whom. Replaying hours of audio to find one moment isn‚Äôt sustainable.

So I built a system to help me: it makes those interviews searchable and easier to interact with, across languages. It helps me zoom in and revisit moments or zoom out and look at a greater picture in their narratives.

I implemented this in a [Kaggle notebook](https://www.kaggle.com/code/arturomp/family-memories). In this post I focus on some implementation bits that are particularly interesting. 

However, I do try to make the whole notebook as easy-to-follow as possible, so check the whole thing out [in Kaggle](https://www.kaggle.com/code/arturomp/family-memories)!

‚∏∫

## üßæ Controlled generation (synthetic interviews)

To simulate interviews (for testing and demoing without using personal data), the system uses Gemini to generate synthetic transcripts using carefully constructed prompts. The generation is tightly controlled via format, speaker turns, and tone:

    transcript_prompt = {
        ...
        "en": f"""
            Imagine a conversation between {speaker_list_en}.
            Make sure that younger people ask questions that advance the interview by either (i) clarifying or seeking to elaborate on what they heard in response from older people, or (ii) asking a new question without switching topics abruptly, that is, without ignoring or not acknowledging what was said.
            Make sure that older people have long turns, each varying from 5 sentences sometimes up to 15 sentences about their life and experiences.
            Only write the dialogue ‚Äî no narration, headings, or character descriptions, inner dialogue or parentheses.  
            Do not include scene-setting, stage directions, speaker bios, or turn counts.
            As in a real interview, interviewees should tell stories with characters to provide context. The stories told should not contradict each other within the interview, although another participant may have a different perspective on the same story.
            Strict format: 
            Each line should begin with the speaker‚Äôs name, followed by a colon and their line.  
            Example:  
            Grandma: [grandma-turn]
            Dad: [dad-turn]
                    Topic:  
            This is an interview-style conversation: a younger person (like a daughter or granddaughter) asks older family members questions about their life.  
            Topics include childhood, family recipes, everyday life in earlier times, traditions, migration, education, work, religion, births, deaths, and how family members met one another.
            Additional instructions:
            - The conversation should feel natural and include personal (but fictional) details.
            - Each person should speak from 20 to 40 times, in no particular order.
        """
    ...
    response = client.models.generate_content(
        model="models/gemini-2.5-flash-preview-04-17", # try 2.5! Better synthetic stories.
        # model="models/gemini-2.0-flash", 
        contents=transcript_prompt,
        config=types.GenerateContentConfig(temperature=0.9)
    )

The result is a fictional yet realistic family conversation that behaves just like real input ‚Äî useful for testing search, retrieval, and evaluation logic without needing actual audio files.

‚∏∫

## üéß Audio understanding (real interviews)

The system transcribes `.mp3` interview recordings using Gemini, with bilingual instructions and optional speaker tagging. Audio files are uploaded and passed as input:

    audio_path = "/kaggle/input/your‚Äëdataset‚Äëname/your_audio.mp3"
    uploaded_audio = client.files.upload(file=audio_path)
    
    ...
    
        transcribe_instruction = {
        ...
        "en": (
            "Transcribe the following audio in English. "
            "If you can identify the speaker's name, use it. If not, use labels like 'Speaker 1:', 'Speaker 2:', etc. "
            "Format each turn as 'Speaker: sentence'. "
            "Only return the transcript ‚Äî no explanations or headings."
        )

Model choice can make a big difference:

    transcribe_response = client.models.generate_content(        
        model="models/gemini-2.5-flash-preview-04-17", # try 2.5! Much more advanced (speaker identification/diarization!) and even translated transcripts.
        # model="models/gemini-2.0-flash",
        contents=[
            transcribe_instruction,
            uploaded_audio
        ],
        config=types.GenerateContentConfig(temperature=0.2)
    )

The result is a clean transcript with speaker turns, suitable for semantic chunking and search.

One of my favorite aspects of this feature is that since modern speech reco systems like Gemini can work with dozens of languages, your interviews can be in [~40](https://ai.google.dev/gemini-api/docs/models#supported-languages) languages supported by Gemini.

Today, different generations of the same family often have different primary languages due to migration, education and many other factors. Powerful, multilingual transcription allows each member to tell their own stories in the language in which they are most comfortable. Our system then allows any other family member, who may or may not share their primary language, to understand more fully those stories. This should have the effect of fostering closer, more authentic connections.

‚∏∫

## üß† Embeddings & vector search

The system embeds transcript chunks using Gemini‚Äôs `text-embedding-004` model, with batching and retry logic.

    is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})
    
    class GeminiEmbeddingFunction(EmbeddingFunction):
        document_mode = True  # Switch to False for queries
    
        @retry.Retry(predicate=is_retriable)
        def __call__(self, input: Documents) -> Embeddings:
            task = "retrieval_document" if self.document_mode else "retrieval_query"
    
            def batch_iterable(iterable, batch_size):
                for i in range(0, len(iterable), batch_size):
                    yield iterable[i:i + batch_size]
    
            all_embeddings = []
    
            for batch in batch_iterable(input, 100):  # Gemini API limit: 100 per batch
                try:
                    response = client.models.embed_content(
                        model="models/text-embedding-004",
                        contents=batch,
                        config=types.EmbedContentConfig(task_type=task),
                    )
                    all_embeddings.extend([e.values for e in response.embeddings])

ChromaDB stores and retrieves results semantically.

    embed_fn = GeminiEmbeddingFunction()
    embed_fn.document_mode = True  # embedding chunks for retrieval
    
    db = chroma_client.get_or_create_collection(
        name=DB_NAME,
        embedding_function=embed_fn
    )
    
    try:
        db.add(documents=documents, ids=[str(i) for i in range(len(documents))])

‚∏∫

## üìö Retrieval-Augmented Generation (RAG)

Relevant chunks are pulled from ChromaDB and added to the prompt for Gemini to generate responses:


    result = db.query(query_texts=[query], n_results=RETRIEVAL_TOP_K)
    [retrieved_docs] = result["documents"]
    
    ...
    
    for doc in retrieved_docs:
        doc_text = doc.replace("\n", " ")
        ref_label = {
            "es": "TEXTO DE REFERENCIA",
            "en": "REFERENCE TEXT"
        }[NOTEBOOK_UI_LANGUAGE]
        prompt += f"\n{ref_label}: {doc_text}"

The prompt itself explicitly tells Gemini to **answer using only the context provided**, and to do so with care and empathy if the information isn‚Äôt sufficient:

    prompt = {
    ...
    "en": f"""Answer the following question using only the context provided below.  
            Imagine you are helping someone understand and recall family memories shared by someone else ‚Äîsuch as a grandparent or relative who was interviewed.  
            If the context does not contain enough information, say so with care and empathy.  
            Be clear, respectful, and kind in your answer.
            
            QUESTION: {query_oneline}
            """

‚∏∫

## üßë‚Äç‚öñÔ∏è Automatic Evaluation

To assess response quality, the system asks Gemini to evaluate each answer in two steps: first, with a structured explanation (groundedness, fluency, and with a comment), and second, with a single overall score using a defined enum schema.

    class AnswerRating(enum.Enum):
        VERY_GOOD = '5'
        GOOD = '4'
        OK = '3'
        BAD = '2'
        VERY_BAD = '1'

Gemini is prompted like this:

    EVAL_PROMPT_TEMPLATE = {
    ... 
        "en": """\
        Evaluate the quality of an AI-generated response based on a question and context.
        
        ## Criteria:
        - Groundedness: Is the answer based on the context?
        - Fluency: Is the answer natural, clear, and coherent?
        
        Rate each from 1 to 5 and include a brief comment. Use the following JSON format:
        
        {{
          "groundedness": [1-5],
          "fluency": [1-5],
          "comment": "Brief comment in English"
        }}
        
        Then, following this rubric, return a **single number** from 1 to 5 as the overall rating:
        
        - 5 (VERY GOOD): if both scores are 5  
        - 4 (GOOD): if both are at least 4  
        - 3 (OK): if both are at least 3  
        - 2 (BAD): if either score is 2  
        - 1 (VERY BAD): if either score is 1
        
        ---
        
        üéØ Original question: {question}
        
        üìö Retrieved context:
        {context}
        
        üìù Generated response:
        {response}
    """

And structured configuration is used to force the response:

    structured_config = types.GenerateContentConfig(
        response_mime_type="text/x.enum",
        response_schema=AnswerRating
    )
    enum_response = chat.send_message(
        message="Give me only the final score (1‚Äì5).",
        config=structured_config
    )

‚∏∫

## üìé Few-shot prompting (optional)

The system includes a short example in the prompt to shape the overall tone and style of the interview. It is left commented out by default as the topics of the example and the short sentences influenced the generation too much and affected flow negatively. That said, they may be useful in certain contexts.

    # You can add the following example to the variable above to perform few-shot prompting. 
    # In my tests it prevents the interview from flowing well.
    # """
    #     Example:
    #     Granddaughter: Grandma, what was your house like when you were a kid?
    #     Grandma: We lived in a small two-bedroom house on the outskirts of town. It had a big front porch, and we didn‚Äôt have air conditioning‚Äîjust fans in the windows. I shared a room with my sister, and our closet was always packed with hand-me-downs. We had a rotary phone in the kitchen, and everyone had to take turns using it. The backyard had a big oak tree we used to climb.
    #     Granddaughter: What did your daily routine look like back then?
    #     Grandma: I‚Äôd wake up early to help pack lunches for school. We walked about twenty minutes, rain or shine. After school, I had chores‚Äîfeeding the chickens, helping with dinner. If there was time, we‚Äôd ride our bikes around the neighborhood or play hopscotch on the driveway. Saturdays were for laundry and church on Sundays.
    #     Granddaughter: Do you remember what kinds of meals you had most often?
    #     Grandma: Oh yes‚Äîlots of casseroles, meatloaf on Mondays, spaghetti on Fridays. My mom made a chicken pot pie from scratch that I‚Äôve never been able to replicate. And we always had Jell-O in the fridge. Every Sunday after church, we‚Äôd have a roast with mashed potatoes and green beans. Those were some of my favorite meals growing up.
    # """

‚∏∫

## üß© Limitations

A kaggle notebook is a starting point, so limitations are plenty. However, here are some that are of immediate concern if you want to use this on your own data:


- **Embedding** unexpectedly lacks speaker awareness despite keeping that information in the chunks (ChromaDB seems to be the culprit, but unclear exactly how yet) - this can affect retrieval when speaker attribution is important
- **Evaluations** vary slightly between runs, but a stronger prompt may help
- **RETRIEVAL_TOP_K** during querying can miss key context if it's too short 

‚∏∫

## üî≠ Future Directions


The system can be expanded with new functions and adaptations. Some current ideas are:

- **Interview assistant:** suggest context-aware follow-up questions during live interviews.  
- **Reusable flow:** adapt the pipeline to other interview corpora, such as [NPR Dialog Transcripts on Kaggle](https://www.kaggle.com/datasets/shuyangli94/interview-npr-media-dialog-transcripts/data).  
- **Personal Oral History Summarizer:** segment interviews into chapters like *Childhood*, *Migration*, *Raising a Family*.  
- **Thematic Indexer:** use embeddings or agents to tag recurring themes (e.g., *resilience*, *food*, *gender roles*) across interviews.  
- **Quote Collage Generator:** surface moving, funny, or surprising quotes and format them for a family memory book.


‚∏∫

## üßµ Conclusion

I hope this project empowers you to start chats with your loved ones about their lives. Through my own experience doing this kind of interviews I've been amazed at how much I've learned and the deeply meaningful moments shared with relatives when I've asked some seemingly simple questions.

If not today, when?

---

# ü´∂ Recuerdos Familiares: Un Sistema Biling√ºe para Explorar y Revivir Entrevistas
[Velo en Kaggle ‚Üí](https://www.kaggle.com/code/arturomp/family-memories)

Tengo varias entrevistas familiares grabadas: conversaciones en las que pregunto a parientes mayores sobre sus vidas. Son incre√≠blemente valiosas para m√≠, para ellos y para toda nuestra familia, y pienso hacer muchas m√°s mientras pueda. Pero como se trata de un proyecto que lleva a√±os, es dif√≠cil recordar qu√© cosas se dijeron y qui√©n las dijo. Reproducir horas de audio para encontrar un momento no es sostenible.

Por eso cre√© este sistema que permite buscar en esas entrevistas e interactuar con ellas m√°s f√°cilmente en varios idiomas. Me ayuda a enfocarme en ciertos momentos o alejarme y ver una imagen m√°s amplia de sus relatos.

Implement√© el sistema en un [cuaderno de Kaggle](https://www.kaggle.com/code/arturomp/family-memories). Aqu√≠ me enfoco en algunas partes de la implementaci√≥n del sistema que son especialmente interesantes.

Lo que s√≠ es que intento que el cuaderno sea lo m√°s f√°cil de seguir y usar posible, ¬°as√≠ que √©chale un ojo [en Kaggle](https://www.kaggle.com/code/arturomp/family-memories)!

‚∏∫

## üßæ Generaci√≥n controlada (entrevistas sint√©ticas)

Para simular las entrevistas (con fines de hacerle pruebas y hacer demos sin utilizar datos personales), el sistema utiliza Gemini para generar transcripciones sint√©ticas con indicaciones cuidadosamente elaboradas. 

La generaci√≥n se controla estrictamente en la indicaci√≥n (o _prompt_) mediante el formato, los turnos del orador y el tono:

    transcript_prompt = {
        "es": f"""
                Imagina una conversaci√≥n entre {speaker_list_es}.
                Aseg√∫rate de que las personas mas j√≥venes hagan preguntas que avanzan la entrevista ya sea (i) aclarando o buscando llegar m√°s a fondo sobre lo que escucharon como respuesta de los mayores, , o (ii) formulando una nueva pregunta sin cambiar bruscamente de tema, es decir, sin ignorar o no reconocer lo que se ha dicho.
                Aseg√∫rate de que las las personas mayores tengan turnos largos, cado uno variando desde 5 enunciados a veces hasta 15 enunciados sobre su vida y sus experiencias.
                Escribe √∫nicamente el di√°logo, sin narraci√≥n, encabezados ni descripciones, di√°logo interior o par√©ntesis.
                No incluyas la escenograf√≠a, las instrucciones de escena, las biograf√≠as de los oradores ni el recuento de turnos.
                Como en una entrevista real, las personas entrevistadas deber√≠an contar historias con personajes para dar contexto. Las historias contadas no deber√≠an contradecirse dentro de la entrevista, aunque otro participante puede tener una perspectiva diferente de la misma historia.
                Formato estricto:  
                Cada l√≠nea debe comenzar con el nombre del hablante, seguido de dos puntos y su intervenci√≥n.  
                Ejemplo:  
                Abuela: [turno-de-la-abuela]  
                Pap√°: [turno-del-pap√°]
                        Tema:  
                Es una conversaci√≥n estilo entrevista: una persona m√°s joven (como una hija o nieta) le hace preguntas a familiares mayores sobre su vida.  
                Tocan temas como la infancia, recetas familiares, el d√≠a a d√≠a en otras √©pocas, tradiciones, migraciones, educaci√≥n, trabajo, religi√≥n, nacimientos, fallecimientos, y c√≥mo se conocieron los miembros de la familia.
                Instrucciones adicionales:
                - La conversaci√≥n debe sonar natural y estar llena de detalles personales (pero inventados).
                - Cada persona debe participar de 20 a 40 veces, sin un orden fijo.
                No agregues encabezados, personajes, narraci√≥n, res√∫menes ni cierres. Solo el di√°logo limpio.
            """,
    ...
    response = client.models.generate_content(
        model="models/gemini-2.5-flash-preview-04-17", # try 2.5! Better synthetic stories.
        # model="models/gemini-2.0-flash", 
        contents=transcript_prompt,
        config=types.GenerateContentConfig(temperature=0.9)
    )

El resultado es una conversaci√≥n ficticia pero realista que se comporta como una entrevista real ‚Äî √∫til para probar b√∫squedas, recuperaci√≥n y evaluaci√≥n sin usar audio real.

‚∏∫

## üéß Transcripci√≥n de audio (entrevistas reales)

El sistema tambi√©n puede transcribir grabaciones `.mp3` usando Gemini, con instrucciones biling√ºes y etiquetas opcionales de hablante:

    audio_path = "/kaggle/input/your‚Äëdataset‚Äëname/your_audio.mp3"
    uploaded_audio = client.files.upload(file=audio_path)
    
    ...
    
        transcribe_instruction = {
        "es": (
            "Transcribe el siguiente audio en espa√±ol. "
            "Si puedes identificar el nombre del hablante, √∫salo. Si no, usa etiquetas como 'Hablante 1:', 'Hablante 2:', etc. "
            "Usa el formato 'Hablante: oraci√≥n' para cada intervenci√≥n. "
            "Responde √∫nicamente con la transcripci√≥n, sin explicaciones ni encabezados."
        ),

La elecci√≥n de modelo puede hacer una gran diferencia:

    transcribe_response = client.models.generate_content(        
        model="models/gemini-2.5-flash-preview-04-17", # try 2.5! Much more advanced (speaker identification/diarization!) and even translated transcripts.
        # model="models/gemini-2.0-flash",
        contents=[
            transcribe_instruction,
            uploaded_audio
        ],
        config=types.GenerateContentConfig(temperature=0.2)
    )

El resultado es una transcripci√≥n con turnos de habla, lista para ser fragmentada y para aplicarle b√∫squeda sem√°ntica.

Uno de mis aspectos favoritos de esta funci√≥n es que, dado que los sistemas modernos de reconocimiento de voz como Gemini pueden trabajar con docenas de idiomas, sus entrevistas pueden ser en [~40](https://ai.google.dev/gemini-api/docs/models#supported-languages) idiomas admitidos por Gemini.

Hoy en d√≠a, distintas generaciones de una misma familia pueden tener diferente lenguas primarias gracias a factores como migraci√≥n, educaci√≥n y otros. El poder de la transcripci√≥n multiling√ºe permite a cada miembro contar sus propias historias en la lengua en la que se sientan m√°s c√≥modos. Nuestro sistema permite entonces que cualquier otro miembro de la familia, sin importar que comparta o no su lengua primaria, comprenda mejor esas historias. Esto fomenta conexiones m√°s aut√©nticas y cercanas.

‚∏∫

## üß† _Embeddings_ y b√∫squeda vectorial

El sistema hace embeddings con fragmentos de la transcripci√≥n utilizando el modelo `text-embedding-004` de Gemini, con l√≥gica de procesamiento por lotes y reintentos.

    is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})
    
    class GeminiEmbeddingFunction(EmbeddingFunction):
        document_mode = True  # Switch to False for queries
    
        @retry.Retry(predicate=is_retriable)
        def __call__(self, input: Documents) -> Embeddings:
            task = "retrieval_document" if self.document_mode else "retrieval_query"
    
            def batch_iterable(iterable, batch_size):
                for i in range(0, len(iterable), batch_size):
                    yield iterable[i:i + batch_size]
    
            all_embeddings = []
    
            for batch in batch_iterable(input, 100):  # Gemini API limit: 100 per batch
                try:
                    response = client.models.embed_content(
                        model="models/text-embedding-004",
                        contents=batch,
                        config=types.EmbedContentConfig(task_type=task),
                    )
                    all_embeddings.extend([e.values for e in response.embeddings])

Luego se almacenan los resultados en ChromaDB para recuperaci√≥n sem√°ntica.

    embed_fn = GeminiEmbeddingFunction()
    embed_fn.document_mode = True  # embedding chunks for retrieval
    
    db = chroma_client.get_or_create_collection(
        name=DB_NAME,
        embedding_function=embed_fn
    )
    
    try:
        db.add(documents=documents, ids=[str(i) for i in range(len(documents))])

‚∏∫

## üìö Generaci√≥n Aumentada por Recuperaci√≥n (RAG por sus siglas en ingl√©s: _Retrieval-Augmented Generation_)

Se recuperan fragmentos relevantes de ChromaDB y se agregan a la indicaci√≥n para que Gemini pueda generar respuestas basadas √∫nicamente en ese contenido:

    result = db.query(query_texts=[query], n_results=RETRIEVAL_TOP_K)
    [retrieved_docs] = result["documents"]
    
    ...
    
    for doc in retrieved_docs:
        doc_text = doc.replace("\n", " ")
        ref_label = {
            "es": "TEXTO DE REFERENCIA",
            "en": "REFERENCE TEXT"
        }[NOTEBOOK_UI_LANGUAGE]
        prompt += f"\n{ref_label}: {doc_text}"

El prompt tambi√©n le indica expl√≠citamente a Gemini que **solo use ese contexto**, y que si no es suficiente, lo diga con empat√≠a:

    prompt = {
    "es": f"""Responde la siguiente pregunta usando solo el contexto proporcionado abajo.  
            Imagina que est√°s ayudando a una persona a comprender y recuperar recuerdos familiares de otra persona ‚Äîcomo un abuelo o familiar entrevistado‚Äî.  
            Si el contexto no contiene suficiente informaci√≥n, ind√≠calo con cuidado y empat√≠a.  
            S√© claro, respetuoso y amable en tu respuesta.
            
            PREGUNTA: {query_oneline}
            """,

‚∏∫

## üßë‚Äç‚öñÔ∏è Evaluaci√≥n Autom√°tica

El sistema le pide a Gemini evaluar las respuestas generadas en dos pasos: primero con una explicaci√≥n estructurada (fundamento, fluidez y con un comentario), y luego con una calificaci√≥n global del 1 al 5, la que se configura con una clase enum:

    class AnswerRating(enum.Enum):
        VERY_GOOD = '5'
        GOOD = '4'
        OK = '3'
        BAD = '2'
        VERY_BAD = '1'

La indicaci√≥n para la evaluaci√≥n es la siguiente:


    EVAL_PROMPT_TEMPLATE = {
    "es": """\
        Eval√∫a la calidad de una respuesta generada por IA en base a una pregunta y contexto.
        
        ## Criterios:
        - Fundamentaci√≥n: ¬øEst√° la respuesta basada en el contexto?
        - Fluidez: ¬øLa respuesta suena natural, clara y coherente?
        
        Califica del 1 al 5 cada uno y proporciona un comentario breve. Usa el siguiente formato JSON:
        
        {{
          "groundedness": [1-5],
          "fluency": [1-5],
          "comment": "Comentario breve en espa√±ol"
        }}
        
        Despu√©s, con base en estas reglas, responde solo con un n√∫mero del 1 al 5 para representar la evaluaci√≥n global:
        
        - 5 (MUY BUENO): si ambas puntuaciones son 5  
        - 4 (BUENO): si ambas son al menos 4  
        - 3 (OK): si ambas son al menos 3  
        - 2 (MALO): si alguna es 2  
        - 1 (MUY MALO): si alguna es 1
        
        ---
        
        üéØ Pregunta original: {question}
        
        üìö Contexto:
        {context}
        
        üìù Respuesta generada:
        {response}
        """,


Y se usa configuraci√≥n estructurada para forzar la respuesta:

    structured_config = types.GenerateContentConfig(
        response_mime_type="text/x.enum",
        response_schema=AnswerRating
    )
    enum_response = chat.send_message(
        message="Give me only the final score (1‚Äì5).",
        config=structured_config
    )

‚∏∫

## üìé Indicaciones con algunos ejemplos (_few-shot prompting_, opcional)

El sistema incluye un ejemplo corto en la indicaci√≥n para dar forma al tono y estilo de la entrevista en general. Se deja comentado por defecto ya que los temas del ejemplo y las breves frases usadas influ√≠an demasiado la generaci√≥n, adem√°s de afectar negativamente el flujo de la entrevista. Dicho eso, pueden ser √∫tiles en ciertos contextos.

        # Puedes a√±adir el siguiente ejemplo a la variable arriba para que la indicaci√≥n tenga 
        # algunos ejemplos (few-shot prompting). En mis pruebas hace que la entrevista no fluya tan bien.
        # """
        #         Ejemplo:
        #         Nieta: Abuela, ¬øc√≥mo era tu casa cuando eras ni√±a?
        #         Abuela: Era una casa muy humilde, con techo de tejas y paredes de adobe. Ten√≠amos solo dos habitaciones para siete personas. No hab√≠a electricidad, y el agua la tra√≠amos del pozo. Recuerdo que en invierno todo se sent√≠a m√°s cerca, porque nos reun√≠amos alrededor de la estufa de le√±a.
        #         Nieta: ¬øY c√≥mo era tu rutina diaria en esa √©poca?
        #         Abuela: Me levantaba antes del amanecer para ayudar a mi mam√° a hacer las tortillas. Luego camin√°bamos casi una hora para llegar a la escuela. En las tardes ayudaba a cuidar a mis hermanos menores y a alimentar a los animales. Apenas si hab√≠a tiempo para jugar.
        #         Nieta: ¬øCu√°l era tu juego favorito?
        #         Abuela: Jug√°bamos a la cuerda y a las escondidas. Pero mi favorito era inventar historias con mis amigas usando mu√±ecas que hac√≠amos nosotras mismas con retazos de tela.
        # """

‚∏∫

## üß© Limitaciones


Un cuaderno kaggle es un punto de partida, por lo que las limitaciones son muchas. Sin embargo, aqu√≠ hay algunas que son de inter√©s inmediato si quieres utilizar esto en tus propios datos:



- Los **embeddings** inesperadamente pierden conocimiento sobre qui√©n dice una oraci√≥na pesar de mantener esa informaci√≥n en los fragmentos (ChromaDB parece ser el culpable, pero a√∫n no est√° claro c√≥mo exactamente), lo que puede afectar a la recuperaci√≥n, sobre todo cuando la atribuci√≥n del hablante es importante
- Las **evaluaciones** var√≠an ligeramente entre ejecuciones, pero un aviso m√°s fuerte puede ayudar.
- **RETRIEVAL_TOP_K** en la consulta puede perder contexto clave si tiene un valor bajos

‚∏∫

##  üî≠ Direcciones futuras

El sistema puede expandirse con nuevas funciones y adaptaciones. Algunas ideas actuales son:

- **Asistente de entrevistas:** sugerencias autom√°ticas de preguntas de seguimiento durante entrevistas reales.  
- **Reutilizaci√≥n del enfoque:** aplicar este flujo a otros conjuntos de datos de entrevistas (como el de [NPR en Kaggle](https://www.kaggle.com/datasets/shuyangli94/interview-npr-media-dialog-transcripts/data)). 
- **Resumen de historias orales:** segmentar entrevistas y resumirlas en cap√≠tulos como *Infancia*, *Migraci√≥n*, *Criar una familia*.  
- **Indexador tem√°tico:** extraer y etiquetar temas frecuentes (como *resiliencia*, *comida*, *g√©nero*) en m√∫ltiples entrevistas.  
- **Generador de citas memorables:** crear collages con frases conmovedoras, graciosas o sorprendentes para incluir en un libro familiar.

‚∏∫

## üßµ Conclusi√≥n

Espero que este proyecto te anime a iniciar pl√°ticas con tus seres queridos sobre sus vidas. En mi propia experiencia con este tipo de entrevistas, me ha sorprendido lo mucho que he aprendido y los momentos que he compartido con mis familiares cuando les he hecho preguntas aparentemente sencillas.

Si no hoy, ¬øcu√°ndo?